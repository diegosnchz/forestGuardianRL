# üîß Detalles T√©cnicos de Implementaci√≥n

## 1. Modificaciones a forest_fire_env.py

### 1.1 Inicializaci√≥n con Zona del R√≠o

```python
def __init__(self, grid_size=10, fire_spread_prob=0.5, initial_trees=0.6, initial_fires=3):
    # ...
    # River zone: Row 0 is a water source (River/Base)
    self.river_row = 0  # ‚Üê Fila 0 es la zona del r√≠o
```

**Impacto:**
- Define d√≥nde est√° la fuente de agua
- Facilita navegaci√≥n (el agente sabe: si row==0, hay agua)

---

### 1.2 Reset con Zona del R√≠o Limpia

```python
def reset(self, seed=None, options=None):
    # ...
    # Clear river zone (row 0) from trees to make it accessible
    self.grid[self.river_row, :] = 0  # ‚Üê Limpia la fila 0
    
    # Ahora place_fires sabe que la fila 0 es segura
```

**Impacto:**
- La fila 0 nunca tiene √°rboles, es siempre segura
- El agente puede llegar al r√≠o sin obst√°culos
- Estrategia clara: cuando tengas miedo, corre al r√≠o

---

### 1.3 Acci√≥n Wait Mejorada

```python
elif action == 6:  # Wait
    if self.agent_pos[0] == self.river_row:
        # En el r√≠o: recarga completa e instant√°nea
        self.water_tank = self.max_water  # De 0 a 10 en 1 paso!
        reward += 2  # Bonificaci√≥n por llegar al agua
    else:
        # En otro lugar: recarga lenta
        self.water_tank = min(self.water_tank + 2, self.max_water)
```

**Impacto:**
- **Costo-Beneficio claro:** Ir al r√≠o es r√°pido pero implica abandona √°rea de fuegos
- **Estrategia emergente:** El agente aprende a equilibrar:
  - Apagar fuegos (consumir agua)
  - Recargar agua (ir al r√≠o)

---

## 2. Arquitectura de train_and_test.py

### 2.1 Clase OperarioAgent (Rule-based)

**Estructura:**
```python
class OperarioAgent:
    def decide_action(obs, agent_pos, water_level, max_water):
        # Examina la observaci√≥n
        # Eval√∫a reglas en orden de prioridad
        # Retorna (action, reason) o (None, reason)
```

**L√≥gica de Decisi√≥n (Orden Cr√≠tico):**

| Prioridad | Condici√≥n | Acci√≥n | Raz√≥n |
|-----------|-----------|--------|-------|
| 1Ô∏è‚É£ **CR√çTICA** | En r√≠o + agua < m√°x | WAIT (6) | Recargar agua |
| 2Ô∏è‚É£ **CR√çTICA** | Fuego adyacente + agua > 0 | EXTINGUISH (5) | Combatir fuego |
| 3Ô∏è‚É£ **ALTA** | Sin agua + hay fuego | MOVE_UP (0) | Huir al r√≠o |
| 4Ô∏è‚É£ **MEDIA** | √Årbol adyacente + fuego + agua baja | CUT (4) | Crear cortafuegos |
| 5Ô∏è‚É£ **BAJA** | Ninguna de arriba | None | Dejar al Navegador |

**Ventaja del Orden:**
- Si estoy en el r√≠o sin agua m√°xima ‚Üí NO me distraer√© apagando fuegos, me recargar√©
- Si tengo agua y hay fuego adyacente ‚Üí COMBATIR ES PRIORITARIO
- Si no tengo agua pero hay fuego ‚Üí HUIR es mejor que quedarse

**Implementaci√≥n del Operario:**
```python
# Detectar fuego adyacente
neighbors = [(row-1,col), (row+1,col), (row,col-1), (row,col+1)]
for n_row, n_col in neighbors:
    if grid[n_row, n_col] == 2:  # 2 = Fire
        adjacent_fire = True
        fire_count_nearby += 1

# Aplicar reglas en orden
if row == 0 and water_level < max_water:
    return 6, "Recargando agua en el r√≠o"
    
if adjacent_fire and water_level > 0:
    return 5, f"Extinguiendo fuego ({fire_count_nearby} fires)"
    
if water_level == 0 and fire_count_nearby > 0:
    return 0, "Sin agua! Navegando al r√≠o"

# ... m√°s reglas ...

# Si nada aplica
return None, "No hay amenaza - Navegante toma control"
```

---

### 2.2 Clase NavegadorAgent (PPO Neural)

**Wrapper alrededor del modelo entrenado:**
```python
class NavegadorAgent:
    def __init__(self, model):
        self.model = model  # Modelo PPO guardado
    
    def decide_action(self, obs):
        action, _states = self.model.predict(obs, deterministic=True)
        return action
```

**Caracter√≠sticas:**
- Usa el modelo PPO ya entrenado
- Determin√≠stico (`deterministic=True`) para reproducibilidad en tests
- Simple: solo predice la acci√≥n

**Qu√© ha Aprendido:**
Despu√©s de 50,000 timesteps, el modelo ha aprendido patrones como:
- "Ac√©rcate a los fuegos para extinguirlos"
- "Si est√°s perdido, ve hacia arriba (r√≠o)"
- "Rodea los √°rboles cuando puedas"
- "Mantente en movimiento para explorar"

---

### 2.3 Clase ForestGuardianManager (Coordinador)

**Flujo Principal:**
```python
def decide_action(obs, agent_pos, water_level, max_water):
    # Paso 1: Consultar Operario
    action, reason = self.operario.decide_action(...)
    
    if action is not None:
        # El Operario tiene una decisi√≥n ‚Üí usar
        self.operario_actions += 1
        return action, "Operario (Rule-based)", reason
    
    else:
        # El Operario no tiene regla ‚Üí usar Navegador
        action = self.navegador.decide_action(obs)
        self.navegador_actions += 1
        return action, "Navegador (PPO)", "Strategic movement"
```

**Estad√≠sticas que Rastrea:**
```python
self.operario_actions       # Cu√°ntas decisiones tom√≥ Operario
self.navegador_actions      # Cu√°ntas decisiones tom√≥ Navegador
self.operario_action_history    # Qu√© decidi√≥ cada vez
self.navegador_action_history   # Qu√© hizo Navegador cada vez
```

**M√©todo de Reporte:**
```python
def print_statistics(self):
    total = self.operario_actions + self.navegador_actions
    operario_pct = (self.operario_actions / total) * 100
    
    # Imprime en formato visual
    print(f"Operario: {self.operario_actions} ({operario_pct:.1f}%)")
    print(f"Navegador: {self.navegador_actions} ({100-operario_pct:.1f}%)")
```

---

## 3. Flujo Completo de Entrenamiento y Prueba

### Fase 1: Entrenamiento del Navegador (50,000 timesteps)

```
1. Crear ForestFireEnv
   ‚îú‚îÄ Grid: 10x10
   ‚îú‚îÄ Fila 0: Zona del r√≠o (sin √°rboles)
   ‚îú‚îÄ Filas 1-9: Bosque con √°rboles y fuegos
   ‚îî‚îÄ Sistema de agua: 0-10 unidades

2. Crear modelo PPO
   ‚îú‚îÄ Policy: "MlpPolicy" (Red Neuronal Simple)
   ‚îú‚îÄ Learning Rate: 3e-4
   ‚îú‚îÄ Pasos por actualizaci√≥n: 2048
   ‚îî‚îÄ √âpocas de entrenamiento: 10

3. Entrenar 50,000 timesteps
   ‚îú‚îÄ El modelo ve miles de estados del bosque
   ‚îú‚îÄ Aprende qu√© acciones generan m√°s recompensa
   ‚îú‚îÄ Gradualmente mejora su estrategia
   ‚îî‚îÄ Se guarda como ppo_forest_fire.zip

Tiempo aprox: 5-10 minutos (dependiendo del hardware)
```

### Fase 2: Evaluaci√≥n con Manager Jer√°rquico

```
Para cada episodio de prueba (3 total):

Loop Principal:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 1. Observar estado actual (grid 10x10)      ‚îÇ
‚îÇ 2. Llamar ForestGuardianManager              ‚îÇ
‚îÇ    ‚îú‚îÄ OperarioAgent eval√∫a reglas           ‚îÇ
‚îÇ    ‚îî‚îÄ NavegadorAgent (si necesario)         ‚îÇ
‚îÇ 3. Ejecutar acci√≥n elegida                  ‚îÇ
‚îÇ 4. Fuego se propaga (50% chance)            ‚îÇ
‚îÇ 5. Registrar recompensa                     ‚îÇ
‚îÇ 6. Contar pasos (termina en 200 o victoria) ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Recopila:
- Total de recompensa por episodio
- Longitud (pasos) de cada episodio
- Porcentaje de uso Operario vs Navegador
- √Årboles y fuegos restantes
```

---

## 4. M√©tricas y KPIs

### 4.1 Recompensa (Reward)

```python
reward = 0

# Durante ejecuci√≥n de acci√≥n
reward += 1        # Si corta un √°rbol
reward += 10       # Si extingue un fuego
reward -= 1        # Si intenta extinguir sin agua
reward += 2        # Si recarga en el r√≠o

# Despu√©s de que el fuego se propaga
reward -= 0.1 * active_fires  # Penalidad por cada fuego activo

# Eventos terminales
reward += 50       # Todos los fuegos extinguidos
reward -= 100      # 80%+ del bosque destruido
```

**Objetivo:** Maximizar recompensa total

---

### 4.2 Episodio (Episode)

**Termina cuando:**
1. ‚úÖ Todos los fuegos se extinguen ‚Üí `reward += 50`
2. ‚ùå 80%+ de √°rboles quemados ‚Üí `reward -= 100`
3. ‚è±Ô∏è 200 pasos ejecutados ‚Üí `truncated = True`

**M√©trica:** `episode_length` (cu√°ntos pasos tard√≥)
- Menos pasos = m√°s eficiente
- Pero debe ser suficiente para ganar

---

### 4.3 Uso del Operario

```python
operario_usage = (operario_actions / total_actions) * 100
```

**Interpretaci√≥n:**
- **0-20%:** Pocos fuegos, navegador hace la mayor√≠a
- **30-50%:** Balance saludable (situaciones variadas)
- **70-100%:** Muchos fuegos (Operario constantemente ocupado)

---

## 5. Flujo de Decisi√≥n Visualizado

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Estado Actual del Entorno               ‚îÇ
‚îÇ (Grid 10x10, agua, posici√≥n)            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               ‚îÇ
               ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ ForestGuardianManager ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               ‚îÇ
               ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ Operario.decide()    ‚îÇ
    ‚îÇ (eval√∫a reglas)      ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚îú‚îÄ ¬øHay regla? 
           ‚îÇ    YES ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
           ‚îÇ              ‚ñº
           ‚îÇ    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
           ‚îÇ    ‚îÇ return (action, msg) ‚îÇ
           ‚îÇ    ‚îÇ  Operario controla  ‚îÇ
           ‚îÇ    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ             ‚îÇ
           NO            ‚îÇ
           ‚îÇ             ‚îÇ
           ‚ñº             ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ Navegador.decide(obs)    ‚îÇ
    ‚îÇ (usa modelo PPO)         ‚îÇ
    ‚îÇ return (action)          ‚îÇ
    ‚îÇ Navegador controla       ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                    ‚îÇ
                    ‚ñº
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ Manager retorna:    ‚îÇ
         ‚îÇ (action, agent_name,‚îÇ
         ‚îÇ  reason)            ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ
                  ‚ñº
       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
       ‚îÇ Ejecutar en Entorno ‚îÇ
       ‚îÇ (env.step(action)) ‚îÇ
       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## 6. Ejemplo de Ejecuci√≥n Paso a Paso

### Escenario: Agent in Danger

```
Estado Inicial:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Agent: (5, 5)           ‚îÇ
‚îÇ Water: 2/10             ‚îÇ
‚îÇ Fuego adyacente: S√ç     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Paso 1: Manager llama Operario
‚îú‚îÄ Detecta: fire at (5,6) [derecha]
‚îú‚îÄ Eval√∫a: adjacent_fire=True, water_level=2
‚îú‚îÄ Regla 2 aplica: "Si fuego adyacente + agua ‚Üí EXTINGUISH"
‚îî‚îÄ Retorna: (5, "Extinguir fuego")

Paso 2: Manager ejecuta acci√≥n 5 (EXTINGUISH)
‚îú‚îÄ Fuego en (5,6) se extingue ‚úì
‚îú‚îÄ Water: 2 ‚Üí 1
‚îî‚îÄ Reward: +10

Paso 3: Fuego se propaga
‚îú‚îÄ Fuego original (4,5) intenta propagarse
‚îú‚îÄ Prob 50%, digamos que se propaga a (3,5)
‚îî‚îÄ Ahora hay fuego en (3,5) (no adyacente)

Paso 4: Manager llama Operario
‚îú‚îÄ Detecta: sin fuego adyacente
‚îú‚îÄ Water: 1 > 0 pero no amenaza inmediata
‚îú‚îÄ Regla 2: NO aplica (sin fuego adyacente)
‚îú‚îÄ Regla 3: NO aplica (water > 0)
‚îú‚îÄ Ninguna regla aplica ‚Üí return None
‚îî‚îÄ Manager cede control al NAVEGADOR

Paso 5: Manager llama Navegador
‚îú‚îÄ Observa: Grid con fuego en (3,5)
‚îú‚îÄ PPO recomienda: acci√≥n 0 (MOVE_UP)
‚îî‚îÄ Retorna: 0

Paso 6: Manager ejecuta acci√≥n 0 (MOVE_UP)
‚îú‚îÄ Agent: (5,5) ‚Üí (4,5)
‚îú‚îÄ Reward: peque√±o penalty por fuego cercano -0.1
‚îî‚îÄ Estado actualizado
```

**Estad√≠sticas despu√©s de estos pasos:**
```
operario_actions: 1  (un paso con Operario)
navegador_actions: 1  (un paso con Navegador)
```

---

## 7. Ventajas T√©cnicas de esta Arquitectura

### Computacionales
```
PPO Puro:
- Predice en CADA paso (red neuronal)
- 50,000 timesteps √ó 1 predicci√≥n = 50k

Jer√°rquico:
- 30% Reglas (muy r√°pido)
- 70% Predicciones (red neuronal)
- 50,000 √ó 0.7 = 35k predicciones (30% m√°s r√°pido!)
```

### Robustez
```
PPO Puro:
- Si el modelo predice "EXTINGUISH" sin agua ‚Üí desperdicio
- Si predice "MOVE AWAY" estando en r√≠o ‚Üí mal entrenado

Jer√°rquico:
- Operario GARANTIZA: "Si agua=0, nunca EXTINGUISH"
- Operario GARANTIZA: "Si en r√≠o, recargar√° agua"
```

### Mantenibilidad
```
PPO Puro:
- Para cambiar "c√≥mo se extinguen fuegos" ‚Üí reentrenar todo

Jer√°rquico:
- Para cambiar "c√≥mo se extinguen fuegos" ‚Üí editar regla en Operario
- R√°pido, no requiere reentrenamiento
```

---

## 8. C√≥mo Debuggear

### Ver qu√© hace cada agente:

```python
# En test_agent, cada 10 pasos:
if steps % 10 == 0:
    action, agent_name, reason = manager.decide_action(...)
    print(f"Step {steps}: {agent_name} ‚Üí {reason}")
    
# Output:
# Step 10: Operario (Rule-based System) ‚Üí Extinguiendo fuego (2 fires)
# Step 20: Navegador (PPO Neural Network) ‚Üí Strategic movement
# Step 30: Operario (Rule-based System) ‚Üí Recargando agua en el r√≠o
```

### Analizar decisiones despu√©s del episodio:

```python
print("Todas las decisiones del Operario:")
for i, decision in enumerate(manager.operario_action_history):
    print(f"  {i}: {decision}")

print("\nTodas las decisiones del Navegador:")
for i, decision in enumerate(manager.navegador_action_history):
    print(f"  {i}: {decision}")
```

---

## 9. Extensiones Futuras

### Easy: Agregar nueva regla al Operario
```python
# En OperarioAgent.decide_action():
# Regla 5 (nueva): Si 3+ √°rboles adyacentes ‚Üí CUT
if tree_count_nearby >= 3:
    return 4, "Creando barrera de seguridad"
```

### Medium: Agregar sub-agente especializado
```python
class BomberoAgent:  # Experto en fuegos grandes
    def decide_action(self, obs, fire_count):
        if fire_count >= 5:
            return ...  # estrategia espec√≠fica

# En Manager:
action1 = self.operario.decide_action(...)
if action1: return action1

action2 = self.bombero.decide_action(...)  # NUEVO
if action2: return action2

action3 = self.navegador.decide_action(...)
return action3
```

### Hard: Entrenar Navegador Especializado
```python
# Navegador para movimiento
self.nav_movement = PPO(...)
self.nav_movement.learn(50000)

# Navegador para combate
self.nav_combat = PPO(...)
self.nav_combat.learn(50000)

# Manager elige el correcto
if water_level > 5:  # Tengo agua
    return self.nav_combat.predict(obs)
else:
    return self.nav_movement.predict(obs)
```

---

**¬°Ahora entiendes c√≥mo funciona cada pieza! üß©**
